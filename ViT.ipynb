{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXCykho37qya",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup cell.\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.transformer_layers import *\n",
    "from utils.captioning_solver_transformer import CaptioningSolverTransformer\n",
    "from utils.classifiers.transformer import CaptioningTransformer\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fczxReWM7qyd"
   },
   "source": [
    "# COCO Dataset\n",
    "As in the previous notebooks, we will use the COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lCKchFVJ7qye"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  /home/jupyter-glaunes/deep_learning_image/ViT/utils/datasets/coco_captioning\n",
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go1XDgIn7qyg"
   },
   "source": [
    "# Transformer\n",
    "In 2017, Vaswani et al introduced the Transformer in their paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) to a) introduce parallelism and b) allow models to learn long-range dependencies. The paper not only led to famous models like BERT and GPT in the natural language processing community, but also an explosion of interest across fields, including vision. While here we introduce the model in the context of image captioning, the idea of attention itself is much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPMDm4F9m0v"
   },
   "source": [
    "# Transformer: Multi-Headed Attention\n",
    "\n",
    "### Dot-Product Attention\n",
    "\n",
    "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\dots,v_n\\}, v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\dots,k_n\\}, k_i \\in \\mathbb{R}^d$, specified as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91g6XLTr7qyi"
   },
   "source": [
    "\\begin{align}\n",
    "c &= \\sum_{i=1}^{n} v_i \\alpha_i \\\\\n",
    "\\alpha_i &= \\frac{\\exp(k_i^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D2wcdeS7qyj"
   },
   "source": [
    "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.\n",
    "\n",
    "### Self-Attention\n",
    "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V,K,Q \\in \\mathbb{R}^{d\\times d}$ to map our input $X$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2JDsjFU7qyl"
   },
   "source": [
    "\\begin{align}\n",
    "v_i = Vx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
    "k_i = Kx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
    "q_i = Qx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5GNjXoW7qyn"
   },
   "source": [
    "### Multi-Headed Scaled Dot-Product Attention\n",
    "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $h$ be number of heads, and $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRyHp98f7qyo"
   },
   "source": [
    "$$\n",
    "Y_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)(XV_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzKHMc3g7qyo"
   },
   "source": [
    "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n",
    "\n",
    "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJFFO4uu7qyp"
   },
   "source": [
    "$$\n",
    "Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KlyhrBW7qyp"
   },
   "source": [
    "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFR0osPf7qyq"
   },
   "source": [
    "\\begin{equation}\n",
    "Y = [Y_1;\\dots;Y_h]A\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4G9fKfW7qyq"
   },
   "source": [
    "were $A \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcDBRnqL9m0w"
   },
   "source": [
    "# Positional Encoding\n",
    "\n",
    "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
    "\n",
    "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zawv8vzV7qyt"
   },
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
    "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0fhlupT7qyt"
   },
   "source": [
    "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkfRjeETn2TU"
   },
   "source": [
    "# Transformer Decoder Block\n",
    "\n",
    "Transformer decoder layer consists of three modules: (1) self attention to process input sequence of vectors, (2) cross attention to process based on available context (i.e. image features in our case), (3) feedforward module to process each vector of the sequence independently.\n",
    "\n",
    "The Transformer decoder layer has three main components: (1) a self-attention module that processes the input sequence of vectors, (2) a cross-attention module that incorporates additional context (e.g., image features in our case), and (3) a feedforward module that independently processes each vector in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZMSyzMWyQpK"
   },
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "[Dosovitskiy et. al.](https://arxiv.org/abs/2010.11929) showed that applying a transformer model on a sequence of image patches (referred to as Vision Transformer) not only achieves impressive performance but also scales more effectively than convolutional neural networks when trained on large datasets. We will train it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAntTB4F0yHC"
   },
   "source": [
    "Vision Transformer converts input image into a sequence of patches of fixed size and embed each patch into a latent vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-equGLTX7rxV"
   },
   "source": [
    "The sequence of patch vectors is processed by transformer encoder layers, each consisting of a self-attention and a feed-forward module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1msDyvLseyI"
   },
   "source": [
    "For classification, ViT divides the input image into patches and processes the sequence of patch vectors using a transformer. Finally, all the patch vectors are average-pooled and used to predict the image class. It uses the same 1D sinusoidal positional encoding to inject ordering information, though 2D sinusoidal and learned positional encodings are also valid choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Redma2A5BZA"
   },
   "source": [
    "\n",
    "We will first test the implementation by overfitting it on one training batch. Tune learning rate and weight decay accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Myf9A3UK4TpM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YU2pQigB9kxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] Loss 2.302585, Top-1 Accuracy: 0.062\n",
      "[10/100] Loss 2.263642, Top-1 Accuracy: 0.219\n",
      "[20/100] Loss 2.212887, Top-1 Accuracy: 0.391\n",
      "[30/100] Loss 2.134503, Top-1 Accuracy: 0.375\n",
      "[40/100] Loss 2.039273, Top-1 Accuracy: 0.422\n",
      "[50/100] Loss 1.921458, Top-1 Accuracy: 0.672\n",
      "[60/100] Loss 1.804003, Top-1 Accuracy: 0.703\n",
      "[70/100] Loss 1.685042, Top-1 Accuracy: 0.781\n",
      "[80/100] Loss 1.623071, Top-1 Accuracy: 0.766\n",
      "[90/100] Loss 1.492154, Top-1 Accuracy: 0.844\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-4  # Experiment with this\n",
    "weight_decay = 1.e-4  # Experiment with this\n",
    "\n",
    "\n",
    "batch = next(iter(DataLoader(train_data, batch_size=64, shuffle=False)))\n",
    "\n",
    "#model = torchvision.models.vit_b_16(image_size=32, num_classes=10)\n",
    "\n",
    "model = torchvision.models.vision_transformer.VisionTransformer(\n",
    "    image_size=32, patch_size=8, num_layers=6, num_heads=4, hidden_dim=128,\n",
    "    mlp_dim=256, num_classes=10)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model.train()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    imgs, target = batch\n",
    "    out = model(imgs)\n",
    "    loss = loss_criterion(out, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    top1 = (out.argmax(-1) == target).float().mean().item()\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"[{epoch}/{epochs}] Loss {loss.item():.6f}, Top-1 Accuracy: {top1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8mrgWcfEE8XE",
    "test": "vit_overfit_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting ViT on one batch. Top-1 accuracy: 0.203125\n"
     ]
    }
   ],
   "source": [
    "# You should get perfect 1.00 accuracy\n",
    "print(f\"Overfitting ViT on one batch. Top-1 accuracy: {top1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI5-AVJNGYS9"
   },
   "source": [
    "Now we will train it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9W2dUKrz8MbG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be24077b3d2b4f36bc50503c90fd6238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ed78f145aa4d9e80e0418260baadf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0140edba745143c6988a8431b8bd741b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aa68c28ad545d5be58d82947ff432c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.classification_solver_vit import ClassificationSolverViT\n",
    "\n",
    "############################################################################\n",
    "# TODO: Train a Vision Transformer model that achieves over 0.45 test      #\n",
    "# accuracy on CIFAR-10 after 2 epochs by adjusting the model architecture  #\n",
    "# and/or training parameters as needed.                                    #\n",
    "#                                                                          #\n",
    "# Note: If you want to use a GPU runtime, go to `Runtime > Change runtime  #\n",
    "# type` and set `Hardware accelerator` to `GPU`. This will reset Colab,    #\n",
    "# so make sure to rerun the entire notebook from the beginning afterward.  #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.0\n",
    "batch_size = 64\n",
    "#model = VisionTransformer()  # You may want to change the default params.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "solver = ClassificationSolverViT(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    model=model,\n",
    "    num_epochs = 2,  # Don't change this\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay,\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "solver.train('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "X1xe22cuGsnw",
    "test": "vit_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.4229\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set: {solver.results['best_test_acc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304, 768])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for p in list(model.encoder.layers.encoder_layer_11.self_attention.parameters()):\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB5dmapnJnQs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
