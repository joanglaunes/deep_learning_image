{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982ecd3c-6e7b-4bf9-9ae2-2b38357e6aa7",
   "metadata": {},
   "source": [
    "Ouvrir ce notebook dans :\n",
    "<a href=\"https://colab.research.google.com/github/joanglaunes/deep_learning_image/blob/main/TP_MNIST_points_init.ipynb\" target=\"_blank\">Google Colab</a>\n",
    "ou\n",
    "<a href=\"https://rosenblatt.ens.math-info.univ-paris5.fr/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fjoanglaunes%2Fdeep_learning_image&urlpath=tree%2Fdeep_learning_image%2FTP_MNIST_points_init.ipynb&branch=main\" target=\"_blank\">Rosenblatt</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e46b46-5d6b-455f-8e42-8c62548a79d2",
   "metadata": {},
   "source": [
    "# TP - Réseaux de neurones pour des données nuages de points\n",
    "\n",
    "Ce TP vise à démontrer les spécificités liées à l\"'apprentissage sur des données de type nuages de points. On va tester\n",
    "deux architectures simples pour la classification sur les images MNIST converties en nuages de points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42f5f3-db08-4ed3-be73-fd3c8c475e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a745c01-11dd-41b6-ad92-033ec6750837",
   "metadata": {},
   "source": [
    "Chargement des données: il s'agit des données MNIST classiques mais qui ont été converties en nuages de points 2D. Chaque donnée est un ensemble de 100 points dans le plan représentant le chiffre manuscrit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13044a91-55f2-41ff-af45-e509fe8cf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points, train_labels = torch.load(\"MNIST_100points_train.pt\")\n",
    "test_points, test_labels = torch.load(\"MNIST_100points_test.pt\")\n",
    "\n",
    "print(train_points.shape, test_points.shape)\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bc491-8c3f-4d2d-84cc-484d75481b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des 5 premières données\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.plot(train_points[i][:,1],train_points[i][:,0],'.')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"label:{train_labels[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca046a-6ae7-409f-aae9-b3214e684c64",
   "metadata": {},
   "source": [
    "## Approche naïve : perceptron multi-couche sur les données\n",
    "\n",
    "__Question 1:__ Ecrire une classe qui implémente un perceptron à trois couches avec les spécificités suivantes:\n",
    "- la dimension d'entrée doit correspondre à $n*d$ où $n=100$ (nombre de points) et $d=2$,\n",
    "- les dimensions des features intermédiaires sont toutes égales à 64,\n",
    "- la dimension de sortie doit être égale au nombre de classes (donc $10$ ici)\n",
    "\n",
    "Quel est le nombre de paramètres de ce modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7997857-a28c-4fd4-a533-a575f4272f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, num_classes):\n",
    "        ### TO DO ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (B,dim_in)\n",
    "        ### TO DO ###\n",
    "        # out.shape = (B,num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5ca08-c1b4-45f6-8cc9-43a266cef0e5",
   "metadata": {},
   "source": [
    "On instancie le modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d205a-4096-4e4c-88a7-89eeb1b4179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = train_points.shape[1:]\n",
    "num_classes = 10\n",
    "model = MLP(n*d,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb236b47-503e-4634-b400-1d106de95d63",
   "metadata": {},
   "source": [
    "Hyperparamètres et fonction pour l'optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e51311-bf57-4af4-b2c1-c27b0bd5ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8407243-a5b5-41c3-9628-05d27edff1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                all_inputs = train_points\n",
    "                all_labels = train_labels\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                all_inputs = test_points\n",
    "                all_labels = test_labels\n",
    "            dataset_size, n, d = all_inputs.shape\n",
    "            n_batches = dataset_size // batch_size\n",
    "            all_inputs = torch.reshape(all_inputs,(n_batches,batch_size,n*d))\n",
    "            all_labels = torch.reshape(all_labels,(n_batches,batch_size))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for k in range(n_batches):\n",
    "                inputs = all_inputs[k,...]\n",
    "                labels = all_labels[k,...]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_acc = running_corrects.double() / dataset_size\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da725e4-02d1-4ca0-ab72-af6324a571d2",
   "metadata": {},
   "source": [
    "On lance l'optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4b375-512e-44c7-8a9a-37e1938b6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, num_epochs=num_epochs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84a8a-024a-4b44-963d-00d511b36cf5",
   "metadata": {},
   "source": [
    "__Question 2:__ Que remarque-t-on ? Essayer d'augmenter le nombre d'époques, la dimension des couches internes, ou le nombre de couches. Comment interpréter les résultats ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f20c3-7445-40c1-8e97-ffa51e86f90b",
   "metadata": {},
   "source": [
    "__Question 3:__ On va à présent implementer une version simplifiée de PointNet, en gardant seulement l'idée essentielle de l'architecture. On reprend le principe d'un perceptron multicouche, mais agissant indépendamment sur chaque point du nuages de points. Ainsi:\n",
    "- la dimension d'entrée doit correspondre à $d=2$,\n",
    "- les dimensions des features intermédiaires sont toutes égales à 64,\n",
    "- une étape de max pooling global est appliqué avant la dernière couche (i.e. on calcule le max terme à terme des $n=100$ features maps)\n",
    "- la dimension de sortie doit être égale au nombre de classes (donc $10$ ici)\n",
    "\n",
    "Quel est le nombre de paramètres de ce modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311b897-ffd3-4ba6-9353-87bd97939f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPointNet(nn.Module):\n",
    "    def __init__(self, n, d, num_classes):\n",
    "        super(BasicPointNet, self).__init__()\n",
    "        ### TO DO ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (B,n,2)\n",
    "        ### TO DO ###\n",
    "        # out.shape = (B,num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcacd2-403f-43d3-bd96-12b94bbd0f48",
   "metadata": {},
   "source": [
    "__Question 4:__ Tester l'entraînement de ce nouveau modèle (attention, ceci suppose de faire une petite modification à la fonction d'entraînement). Que remarque-t-on ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c67628-4e4a-4770-b13c-c1a28b93fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicPointNet(n,d,num_classes).to(device)\n",
    "train_model(model, criterion, optimizer, num_epochs=num_epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bde96-039c-4144-80ca-360b4f71af4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
